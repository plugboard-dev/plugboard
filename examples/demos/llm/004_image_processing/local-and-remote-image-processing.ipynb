{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Local and Remote Image Processing\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/plugboard-dev/plugboard/blob/main/examples/demos/llm/004_image_processing/local-and-remote-image-processing.ipynb)\n",
    "\n",
    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/plugboard-dev/plugboard)\n",
    "\n",
    "This example demonstrates a hybrid AI pipeline that combines a lightweight local model with a powerful remote LLM.\n",
    "\n",
    "**The Goal:**\n",
    "\n",
    "We want to process a stream of images, identify which ones contain clocks, and then read the time from those clocks.\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "Sending every image to a large multimodal model (like OpenAI) is expensive and slow.\n",
    "\n",
    "**The Solution:**\n",
    "\n",
    "1.  Use a fast, local computer vision model (MobileNetV3) to classify images.\n",
    "2.  Filter the stream:\n",
    "    *   If it's a clock -> Send to GPT-5-nano to read the time.\n",
    "    *   If it's not a clock -> Log it and skip the expensive call.\n",
    "\n",
    "This architecture demonstrates **Event-driven routing** in Plugboard, where data flows to different downstream components based on analysis results.\n",
    "\n",
    "The overall model will look like this:\n",
    "\n",
    "![](https://mermaid.ink/img/pako:eNrVld9KwzAUxl8l5LJsPkCRoQwFoRXZjTdCOUvObDFNSpo5ofTdXZP-ddJtrrvwtvm-c37nOyEtKFMcqU83Qu1YDNqQYPUmCdEIHPVdQfIYMvSJVlvJkc-IgDUKnzwmAldWc7vWC89zes8jJZnPF0SocXuwP39K4R2d28krd9X7z2bbmgnI82STjFdgIML95MKV6DwNw4XzsxjZx5h_WQmWdVsGJlGyRnHOhuPyaablScGwOAIOmRkrF1aye6dydQbG7p6INMq0YpjnamzCILQLf2mk9eb77vb2TFvSYjr4nU7MsTvxajX9kZ3rSvlNByaVjE6Ee1bykO_A3zJ2J-dy_nT-R1b7rSsT49dwyQ-fKE3V_aZd5xS3YxDT7-2baAYEV4nWPmRTvEC9lI5HOnHX89KkM5qiTiHh1C-oiTGt_rIcN7AVhpblN21ynjw=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from io import BytesIO\n",
    "import typing as _t\n",
    "from datetime import time\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, ConfigDict, AnyUrl\n",
    "from PIL import Image\n",
    "import httpx\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from plugboard.component import Component\n",
    "\n",
    "from plugboard.connector import AsyncioConnector, ConnectorBuilder\n",
    "from plugboard.component import IOController as IO\n",
    "from plugboard.schemas import ComponentArgsDict, ConnectorSpec\n",
    "from plugboard.process import LocalProcess\n",
    "from plugboard.library import FileReader, FileWriter, LLMImageProcessor\n",
    "from plugboard.events import Event, EventConnectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Define Local Components\n",
    "\n",
    "First, we define the components that run locally.\n",
    "\n",
    "*   `LoadImage`: Downloads an image from a URL and converts it to a format suitable for processing.\n",
    "*   `LocalModel`: Uses `torchvision`'s pre-trained **MobileNetV3** to classify the image. This runs entirely on your machine (or CPU/GPU) and is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImage(Component):\n",
    "    \"\"\"Loads an image from a URL\"\"\"\n",
    "\n",
    "    io = IO(inputs=[\"url\"], outputs=[\"image\"])\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"PlugboardExample/1.0 (https://docs.plugboard.dev, hello@plugboard.dev)\"\n",
    "        }\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            r = await client.get(self.url, headers=headers, follow_redirects=True)\n",
    "            r.raise_for_status()\n",
    "\n",
    "        self.image = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class LocalModel(Component):\n",
    "    \"\"\"Passes an image into a MobileNetV3 model\"\"\"\n",
    "\n",
    "    io = IO(inputs=[\"image\"], outputs=[\"classification\"])\n",
    "\n",
    "    async def init(self) -> None:\n",
    "        self._model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        self._categories = MobileNet_V3_Small_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "        self._model.eval()\n",
    "        self._transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),  # MobileNetV3 expects 224x224 input\n",
    "                transforms.ToTensor(),  # Convert PIL Image to torch.Tensor (CHW format, [0.0,1.0] range)\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],  # Normalize to ImageNet means\n",
    "                    std=[0.229, 0.224, 0.225],  # Normalize to ImageNet stds\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        tensor_image = self._transform(self.image)\n",
    "        batch_tensor = tensor_image.unsqueeze(0)  # Add batch dimension\n",
    "        outputs = self._model(batch_tensor).squeeze(0).softmax(0)\n",
    "        self.classification = dict(zip(self._categories, outputs.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Define Events\n",
    "\n",
    "We define two types of events to handle the routing logic:\n",
    "\n",
    "*   `MatchEvent`: Fired when the image matches our criteria (it is a clock). It carries the image data.\n",
    "*   `NonMatchEvent`: Fired when the image does not match. It carries the classification results for logging.\n",
    "\n",
    "We use Pydantic models (`MatchData`, `NonMatchData`) to strictly type the event payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchData(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    url: AnyUrl\n",
    "\n",
    "\n",
    "class MatchEvent(Event):\n",
    "    type: _t.ClassVar[str] = \"match\"\n",
    "    data: MatchData\n",
    "\n",
    "\n",
    "class NonMatchData(BaseModel):\n",
    "    classification: dict[str, float]\n",
    "    url: AnyUrl\n",
    "\n",
    "\n",
    "class NonMatchEvent(Event):\n",
    "    type: _t.ClassVar[str] = \"non_match\"\n",
    "    data: NonMatchData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Logic and Adapters\n",
    "\n",
    "Now we define the components that use these events.\n",
    "\n",
    "*   `CheckClassification`: Takes the classification result from `LocalModel`. If the probability of any target class (e.g., \"analog clock\") is above a threshold, it fires a `MatchEvent`. Otherwise, it fires a `NonMatchEvent`.\n",
    "*   `MatchAdapter` & `NonMatchAdapter`: These components subscribe to the events and convert the event payload back into a standard output stream. This allows us to connect them to downstream components like `LLMImageProcessor` or `FileWriter`.\n",
    "\n",
    "**Note:** `MatchAdapter` also passes on the URL of time image to the `LLMImageProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckClassification(Component):\n",
    "    \"\"\"Checks for a classification match.\"\"\"\n",
    "\n",
    "    io = IO(inputs=[\"url\", \"classification\"], output_events=[MatchEvent, NonMatchEvent])\n",
    "\n",
    "    def __init__(\n",
    "        self, classes: list[str], threshold: float = 0.5, **kwargs: _t.Unpack[ComponentArgsDict]\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._class_names = classes\n",
    "        self._threshold = threshold\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        match = False\n",
    "        for c in self._class_names:\n",
    "            if self.classification.get(c, 0) >= self._threshold:\n",
    "                match = True\n",
    "                break\n",
    "        if match:\n",
    "            self.io.queue_event(MatchEvent(source=self.name, data=MatchData(url=self.url)))\n",
    "        else:\n",
    "            self.io.queue_event(\n",
    "                NonMatchEvent(\n",
    "                    source=self.name,\n",
    "                    data=NonMatchData(classification=self.classification, url=self.url),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MatchAdapter(Component):\n",
    "    \"\"\"Listens for match events and passes them on for further processing.\"\"\"\n",
    "\n",
    "    io = IO(outputs=[\"url\"], input_events=[MatchEvent])\n",
    "\n",
    "    def __init__(self, **kwargs: _t.Unpack[ComponentArgsDict]) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._buffer = []\n",
    "\n",
    "    @MatchEvent.handler\n",
    "    async def handle_match(self, event: MatchEvent):\n",
    "        self._buffer.append(event.data)\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        if self._buffer:\n",
    "            data = self._buffer.pop(0)\n",
    "            self.url = str(data.url)\n",
    "\n",
    "\n",
    "class NonMatchAdapter(Component):\n",
    "    \"\"\"Listens for non-match events and passes them on for further processing.\"\"\"\n",
    "\n",
    "    io = IO(outputs=[\"classification\", \"url\"], input_events=[NonMatchEvent])\n",
    "\n",
    "    def __init__(self, **kwargs: _t.Unpack[ComponentArgsDict]) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._buffer = []\n",
    "\n",
    "    @NonMatchEvent.handler\n",
    "    async def handle_non_match(self, event: NonMatchEvent):\n",
    "        self._buffer.append(event.data)\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        if self._buffer:\n",
    "            data = self._buffer.pop(0)\n",
    "            self.classification = data.classification\n",
    "            self.url = str(data.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. LLM Configuration\n",
    "\n",
    "We define a Pydantic model for the LLM's response. This ensures we get structured data (JSON) back from the model, rather than just free text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeReading(BaseModel):\n",
    "    time: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Define and Run Process\n",
    "\n",
    "Finally, we wire everything together in a `LocalProcess`.\n",
    "\n",
    "1.  **Input**: We create a CSV with URLs of images (some clocks, some not).\n",
    "2.  **Pipeline**:\n",
    "    *   `FileReader` -> `LoadImage` -> `LocalModel` -> `CheckClassification`\n",
    "    *   `CheckClassification` fires events.\n",
    "    *   `MatchEvent` -> `MatchAdapter` -> `LLMImageProcessor` -> `FileWriter` (matches.csv)\n",
    "    *   `NonMatchEvent` -> `NonMatchAdapter` -> `FileWriter` (non_matches.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = lambda src, dst: AsyncioConnector(spec=ConnectorSpec(source=src, target=dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV with some image URLs\n",
    "images_df = pd.DataFrame(\n",
    "    {\n",
    "        \"url\": [\n",
    "            # This is a clock\n",
    "            \"https://images.unsplash.com/photo-1541480601022-2308c0f02487?ixlib=rb-4.1.0&q=85&fm=jpg&crop=entropy&cs=srgb\",\n",
    "            # Not a clock (Train)\n",
    "            \"https://images.unsplash.com/photo-1535535112387-56ffe8db21ff?ixlib=rb-4.1.0&q=85&fm=jpg&crop=entropy&cs=srgb\",\n",
    "            # This is a clock\n",
    "            \"https://images.unsplash.com/photo-1563861826100-9cb868fdbe1c?ixlib=rb-4.1.0&q=85&fm=jpg&crop=entropy&cs=srgb\",\n",
    "            # Not a clock (Dog)\n",
    "            \"https://images.unsplash.com/photo-1530281700549-e82e7bf110d6?ixlib=rb-4.1.0&q=85&fm=jpg&crop=entropy&cs=srgb\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "images_df.to_csv(\"images.csv\", index=False)\n",
    "\n",
    "# Define Components\n",
    "components = [\n",
    "    FileReader(name=\"reader\", path=\"images.csv\", field_names=[\"url\"]),\n",
    "    LoadImage(name=\"loader\"),\n",
    "    LocalModel(name=\"classifier\"),\n",
    "    CheckClassification(name=\"checker\", classes=[\"analog clock\", \"wall clock\", \"digital clock\"]),\n",
    "    MatchAdapter(name=\"match_adapter\"),\n",
    "    LLMImageProcessor(\n",
    "        name=\"llm_processor\",\n",
    "        prompt=\"Read the time on the clock\",\n",
    "        response_model=TimeReading,\n",
    "        expand_response=True,\n",
    "        llm_kwargs={\"model\": \"gpt-4o-mini\"},\n",
    "    ),\n",
    "    FileWriter(name=\"match_writer\", path=\"matches.csv\", field_names=[\"time\", \"url\"]),\n",
    "    NonMatchAdapter(name=\"non_match_adapter\"),\n",
    "    FileWriter(\n",
    "        name=\"non_match_writer\", path=\"non_matches.csv\", field_names=[\"classification\", \"url\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define Connectors\n",
    "connectors = [\n",
    "    connect(\"reader.url\", \"loader.url\"),\n",
    "    connect(\"loader.image\", \"classifier.image\"),\n",
    "    connect(\"reader.url\", \"checker.url\"),\n",
    "    connect(\"classifier.classification\", \"checker.classification\"),\n",
    "    # Adapter to Processor/Writer\n",
    "    connect(\"match_adapter.url\", \"llm_processor.image\"),\n",
    "    connect(\"llm_processor.time\", \"match_writer.time\"),\n",
    "    connect(\"match_adapter.url\", \"match_writer.url\"),\n",
    "    connect(\"non_match_adapter.classification\", \"non_match_writer.classification\"),\n",
    "    connect(\"non_match_adapter.url\", \"non_match_writer.url\"),\n",
    "]\n",
    "\n",
    "# Event Connectors\n",
    "builder = ConnectorBuilder(connector_cls=AsyncioConnector)\n",
    "event_builder = EventConnectorBuilder(connector_builder=builder)\n",
    "event_connectors = list(event_builder.build(components).values())\n",
    "\n",
    "# Define Process\n",
    "process = LocalProcess(components=components, connectors=connectors + event_connectors)\n",
    "\n",
    "async with process:\n",
    "    await process.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now open the output CSVs to see the time read from the clock images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat matches.csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
