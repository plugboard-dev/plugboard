{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multiple LLMs for data cleaning\n",
    "\n",
    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/plugboard-dev/plugboard)\n",
    "\n",
    "This model demonstrates how you can use multiple LLMs for cleaning and organising input data. We're going to use the hotel review dataset, but this approach could easily be applied to other data structuring tasks.\n",
    "\n",
    "Using a costly LLM...\n",
    "\n",
    "To run this model you will need to set environment variables for [`OPENAI_API_KEY`](https://platform.openai.com/settings/) and [`GOOGLE_API_KEY`](https://aistudio.google.com/app/apikey). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "import typing as _t\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from plugboard.component import Component, IOController as IO\n",
    "from plugboard.connector import AsyncioConnector\n",
    "from plugboard.connector import AsyncioConnector, ConnectorBuilder\n",
    "from plugboard.events import Event, EventConnectorBuilder, StopEvent\n",
    "from plugboard.schemas import ConnectorSpec\n",
    "from plugboard.process import LocalProcess\n",
    "from plugboard.library import FileReader, FileWriter, LLMChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandlingComponent(Component, ABC):\n",
    "    base_class: type[Component]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name=kwargs[\"name\"])\n",
    "        kwargs[\"name\"] = f\"_event_{kwargs['name']}\"\n",
    "        self._base = self.base_class(**kwargs)\n",
    "\n",
    "    async def init(self):\n",
    "        await self._base.init()\n",
    "\n",
    "    async def step(self):\n",
    "        pass\n",
    "\n",
    "    async def destroy(self):\n",
    "        await self._base.destroy()\n",
    "        return await super().destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingFileReader(EventHandlingComponent):\n",
    "    io = IO(outputs=[\"review_id\", \"review_text\"])\n",
    "    base_class = FileReader\n",
    "\n",
    "    async def step(self):\n",
    "        await self._base.step()\n",
    "        for field in self._base.io.outputs:\n",
    "            setattr(self, field, getattr(self._base, field))\n",
    "        if self._base.io.is_closed:\n",
    "            await self.send(StopEvent())\n",
    "\n",
    "\n",
    "file_reader = StoppingFileReader(\n",
    "    name=\"file-reader\", path=\"hotel-reviews.csv\", field_names=[\"review_id\", \"review_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating = _t.Literal[\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "\n",
    "class HotelReview(BaseModel):\n",
    "    facilities: Rating\n",
    "    cleanliness: Rating\n",
    "    location: Rating\n",
    "    price: Rating\n",
    "    staff: Rating\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are going to receive hotel reviews. For each one try to identify if the following aspects are positive, negative or neutral:\n",
    "facilities, cleanliness, location, price, staff.\n",
    "If the review doesn't mention a particular aspect, respond with 'neutral'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_mini = LLMChat(\n",
    "    name=\"openai-mini\",\n",
    "    system_prompt=system_prompt,\n",
    "    llm_kwargs={\"model\": \"gpt-4o-mini\"},\n",
    "    response_model=HotelReview,\n",
    ")\n",
    "gemini_mini = LLMChat(\n",
    "    name=\"gemini-lite\",\n",
    "    llm=\"llama_index.llms.gemini.Gemini\",\n",
    "    system_prompt=system_prompt,\n",
    "    llm_kwargs={\"model\": \"models/gemini-2.0-flash-lite\"},\n",
    "    response_model=HotelReview,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveReview(BaseModel):\n",
    "    \"\"\"Data to save from review.\"\"\"\n",
    "\n",
    "    review_id: str\n",
    "    review: HotelReview\n",
    "\n",
    "\n",
    "class RawReview(BaseModel):\n",
    "    \"\"\"Raw review data for the expensive LLM.\"\"\"\n",
    "\n",
    "    review_id: str\n",
    "    review_text: str\n",
    "\n",
    "\n",
    "class SaveReviewEvent(Event):\n",
    "    \"\"\"Event to save a review.\"\"\"\n",
    "\n",
    "    type: _t.ClassVar[str] = \"save_review\"\n",
    "    data: SaveReview\n",
    "\n",
    "\n",
    "class RequestReviewEvent(Event):\n",
    "    \"\"\"Event to request another LLM review.\"\"\"\n",
    "\n",
    "    type: _t.ClassVar[str] = \"request_review\"\n",
    "    data: RawReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareLLMs(Component):\n",
    "    \"\"\"Checks if two LLMs give the same output.\"\"\"\n",
    "\n",
    "    io = IO(\n",
    "        inputs=[\"review_id\", \"review_text\", \"model1\", \"model2\"],\n",
    "        output_events=[SaveReviewEvent, RequestReviewEvent],\n",
    "    )\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        if self.model1 == self.model2:\n",
    "            # Both light models agree, save the review\n",
    "            response = HotelReview.model_validate_json(self.model1)\n",
    "            self.io.queue_event(\n",
    "                SaveReviewEvent(\n",
    "                    source=self.name, data=SaveReview(review_id=self.review_id, review=response)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Request a review from the expensive model\n",
    "            self._logger.warning(\n",
    "                \"Models disagree, requesting review from expensive model\", review_id=self.review_id\n",
    "            )\n",
    "            self.io.queue_event(\n",
    "                RequestReviewEvent(\n",
    "                    source=self.name,\n",
    "                    data=RawReview(review_id=self.review_id, review_text=self.review_text),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "compare = CompareLLMs(name=\"compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMChatOnEvent(EventHandlingComponent):\n",
    "    \"\"\"Runs an LLM on a review event.\"\"\"\n",
    "\n",
    "    io = IO(input_events=[RequestReviewEvent], output_events=[SaveReviewEvent])\n",
    "    base_class = LLMChat\n",
    "\n",
    "    @RequestReviewEvent.handler\n",
    "    async def call_llm(self, event: RequestReviewEvent) -> None:\n",
    "        self._base.prompt = event.data.review_text\n",
    "        await self._base.step()\n",
    "        response = HotelReview.model_validate_json(self._base.response)\n",
    "        save_event = SaveReviewEvent(\n",
    "            source=self.name, data=SaveReview(review_id=event.data.review_id, review=response)\n",
    "        )\n",
    "        self.io.queue_event(save_event)\n",
    "\n",
    "\n",
    "open_ai_expensive = LLMChatOnEvent(\n",
    "    name=\"openai-expensive\",\n",
    "    system_prompt=system_prompt,\n",
    "    llm_kwargs={\"model\": \"gpt-4o\"},\n",
    "    response_model=HotelReview,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptureData(Component):\n",
    "    \"\"\"This component captures the data.\"\"\"\n",
    "\n",
    "    io = IO(inputs=[\"source\", \"review\"], input_events=[SaveReviewEvent])\n",
    "\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._path = path\n",
    "        self._data = []\n",
    "\n",
    "    async def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @SaveReviewEvent.handler\n",
    "    async def car_leaves_wash(self, event: SaveReviewEvent) -> None:\n",
    "        data = event.data\n",
    "        self._data.append({\"source\": event.source, \"review\": data.model_dump_json()})\n",
    "\n",
    "    async def destroy(self):\n",
    "        pd.DataFrame(self._data).to_csv(self._path, index=False)\n",
    "        return await super().destroy()\n",
    "\n",
    "\n",
    "file_writer = CaptureData(name=\"file-writer\", path=\"processed-reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [\n",
    "    file_reader,\n",
    "    open_ai_mini,\n",
    "    gemini_mini,\n",
    "    compare,\n",
    "    open_ai_expensive,\n",
    "    file_writer,\n",
    "]\n",
    "connect = lambda in_, out_: AsyncioConnector(spec=ConnectorSpec(source=in_, target=out_))\n",
    "connectors = [\n",
    "    connect(\"file-reader.review_text\", \"openai-mini.prompt\"),\n",
    "    connect(\"file-reader.review_text\", \"gemini-lite.prompt\"),\n",
    "    connect(\"file-reader.review_id\", \"compare.review_id\"),\n",
    "    connect(\"file-reader.review_text\", \"compare.review_text\"),\n",
    "    connect(\"openai-mini.response\", \"compare.model1\"),\n",
    "    connect(\"gemini-lite.response\", \"compare.model2\"),\n",
    "]\n",
    "connector_builder = ConnectorBuilder(connector_cls=AsyncioConnector)\n",
    "event_connector_builder = EventConnectorBuilder(connector_builder=connector_builder)\n",
    "event_connectors = list(event_connector_builder.build(components).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = LocalProcess(\n",
    "    components=components,\n",
    "    connectors=connectors + event_connectors,\n",
    ")\n",
    "async with process:\n",
    "    await process.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
